넷마블 특강
-----------------------------------------------------------------------------------------------------------------------------
1. WHY (목적)
현황 파악을 통한 문제 도출

2. HOW (방법)
why에 근거한 문제 해결 방법

3. WHAT (결과)
why의 결과로 나오는 최종 프로덕
-----------------------------------------------------------------------------------------------------------------------------
학교 막 졸업한 애들

"A가 B보다 많이 좋다"고 표현하는 애들 가르치기 어렵다
-> 통계를 현실에서 써보지 않아서 이렇게 표현한다
-> 통계량과 가설검정에 기반에 얘기를 해야 이해가 잘 안 돼도 좀 똑똑한 애가 말을 하나보다 이해한다고...
-----------------------------------------------------------------------------------------------------------------------------

분석할 때 어떤 방법론을 많이 쓰나요?
1. 데이터 전처리
2. 전통적인 통계 분석 방법론
3. 머신 러닝
4. 딥 러닝
5. 데이터 시각화
-----------------------------------------------------------------------------------------------------------------------------
이때 [1. 데이터 전처리]를 정말 잘 준비해야하고, [2. 전통적인 통계 분석 방법론]도 중요하다
면접 때 자기는 F검정, t검정, ANOVA 이런거 물어본다
-----------------------------------------------------------------------------------------------------------------------------
No silver bullet
가장 효과적이면서도 효율적인 방법은 없음
-----------------------------------------------------------------------------------------------------------------------------
<무엇을 배워야 하나?>
1. 데이터를 coolumn 단위로 보고 분석할 수 있는가?

2. SQL, SQL, SQL
    - 가능한 부분은 SQL에서 처리할 수 있도록 훈련 필요
         - SQL: 대규모 시스템으로 빠른 처리 가능(예. BigQuery, SnowFlake, RedShift, ...)

3. 익숙하지 않은 데이터라면 Excel로 데이터 디테일 확인하고 prototyping
    - column 내에 어떤 내용이 있는지 엑셀과 같은 스프레드시트로 확인
    - 데이터베이스 연결 기능을 이용하여 엑셀에서

4. Python 이용한 전처리는 비용(시간, 가격, 데이터 입출력)이 더 필요함
    - 그래도 어쩔 수 없이 사용해야 하는 경우 발생
    - Pandas는 필수
    - column 단위 처리를 위해 for-loop 돌리는 일은 없어야 함
        - 대신 apply, lambda 함수 사용

-----------------------------------------------------------------------------------------------------------------------------
<데이터 사이언티스트가 꼭 할 수 있어야 하는 스킬들>
1. 데이터 전처리
    - 10만 줄 이하: 엑셀로도 OK
    - 1000만 줄 (101GB) 미만: 내 컴퓨터에 설치한 RDBMS, NoSQL로 OK
    - 그 이상: Cloud DW, Spark clusters, Hive clusters

2. 데이터 다루기
    - 엑셀 함수 (clookup, if, iferror, sumif, countif, ...)
    - SQL ( select 문, 서브쿼리, 최적화 방법)
    - BigQuery ( Standard SQL, BigQuery on jupyter notebook, Airflow 2, ...)
  
3. 데이터 분석
    - SQL (aggregation), BigQuery
    - Python (R, Java, ... 데이터 사이언티스트 실무에서 거의 사용하지 않습니다.)
        - Pandas, scikit-learn, statsmodel, tensorflow, ...
    - 데이터 시각화 on python
        - matplotlib, seaborn, pandas profiling, ...
-----------------------------------------------------------------------------------------------------------------------------
질문 및 답변 (1/3)

1. 무엇을 알아야 할까요?
    - 필수: 엑셀, SQL, Python, Pandas, scikit-learn
    - 옵션1: 컴퓨터 아키텍처, (Python)numpy, lambda function, BigQuery, statsmodel
    - 옵션2: linux 기본 명령들, (python)병렬 프로세싱, GCP command tools, tensorflow, keras
    
2. R이나 NoSQL에 대해 잘 알고 있어도 도움이 될까요?
    - 답변: R을 사용하는 회사는 별로 없습니다. 취직이나 이직에 큰 도움이 되지 않습니다.
    - 답변: NoSQL은 필요할 때 배워서 사용하시면 됩니다.
    - 답변: Spark, Hive 등의 분산처리 스킬은 필수가 아닙니다.
    
3. pytorch를 쓰면 안되나요?
    - 회사에서 일반적으로 tensorflow를 사용합니다. 써도 되지만 TF 다시 공부해야 할 가능성이 훨씬 큽니다.

4. 추가적으로 공부해야 할 것이 있나요?
    - 계량경제학 기반의 인과관계 분석에 대해 공부하면 좋다.
-----------------------------------------------------------------------------------------------------------------------------
lambda function 을 왜 쓰는지 뭐가 좋은지 for-loop보다 어떻게 좋은지 설명, 이해하고 있어야 한다!!!!!!!!!
-----------------------------------------------------------------------------------------------------------------------------
질문 및 답변 (2/3)

클라우드 관련 자격증은 미리 따는 것은 효용이 떨어진다고 생각합니다.

캐글이나 데이콘 같은 거 열심히 하시면 좋습니다.


캐글을 나가서 입상하라는게 아니라 다양한 데이터를 만지고 공부를 해봤으면 하는 마음이다.

캐글의 데이터는 전처리가 어느정도 되어 있는 상태이기 때문에 전처리를 제외한 알고리즘이나 머신러닝에 집중해서 공부하기에 좋다.


-----------------------------------------------------------------------------------------------------------------------------
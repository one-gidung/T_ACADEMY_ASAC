환경변수
1. JAVA_HOME  C:\Program Files\Java\jdk1.8.0_211
2. HADOOP_HOME d:\hadoop-2.6.0
3. SPARK_HOME D:\spark-2.4.6-bin-hadoop2.7
4. PYTHONPATH  %PYTHONPATH%;D:\spark-2.4.6-bin-hadoop2.7\python
D:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip

5. path추가
D:\hadoop-2.6.0\bin
D:\spark-2.4.6-bin-hadoop2.7\bin


하둡환경
1.
D:\hadoop-2.6.0\etc\hadoop\hadoop-env.cmd
에 자바패스 수정
set JAVA_HOME=C:\Progra~1\Java\jdk1.8.0_251
2. core-site.xml (하둡시스템 접근 아이피와 포트 지정)

<configuration>
	<property>
   		<name>fs.default.name</name>
   		<value>hdfs://localhost:9000</value>
	</property> 
</configuration>

3. hdfs-site.xml
<configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
</property>

<property>
    <name>dfs.permissions</name>
    <value>false</value>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>/d:/hadoop/data/namenode</value>
 </property>
 <property>
  <name>dfs.datanode.data.dir</name>
  <value>/d:/hadoop/data/datanode</value>
 </property>
</configuration>

4. 하둡파일시스템 포맷
hdfs namenode -format
start-all.cmd ( start-dfs.cmd, start-yarn.cmd)
hdfs dfs –cmd <args> : cmd args 와 동일하다

hdfs dfs –ls / :root 경로의 파일및 딕렉터리 show
hdfs dfs –mkdir /data : data 경로 생성
hdfs dfs –put aa.txt /data :aa.txt 하둡파일시스템으로 복사
hdfs dfs -cat /data/aa.txt

프로세서 종료
stop-all.cmd

